{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c58640",
   "metadata": {},
   "source": [
    "#### Programming Assignment – Chatbot Eliza\n",
    "Write an Eliza program in Python. No chat packages/functions/libraries are allowed to use. The program should be called eliza.py, and it should run from the command line with no arguments. NOTE: if you use **jupyter notebook**, you can have all comments and required running outputs/logs in the notebook file(s). And then save into HTML(s) and zip all notebook file(s), HTML files, and other files into ONE zip file. Please submit only one zip file.\n",
    "\n",
    "Your program should engage in a dialogue with the user, with your program Eliza playing the role of a psychotherapist. Your program should be able carry out \"word spotting\", that is it should recognize certain key words and respond simply based on that word being present in the input. It should also be able to transform certain simple sentence forms from statements (from the user) into questions (that Eliza will ask). Also, try to personalize the dialogue by asking and using the user's name. \n",
    "\n",
    "In addition, your program should be robust. If the user inputs gibberish or a very complicated question, Eliza should respond in some plausible way (I didn't quite understand, can you say that another way, etc.). “Word spotting”, sentence transformation, and robustness are the minimum requirements for your code. You can implement additional functionalities, inspired by the dialogues presented in Weizenbaum paper. You may receive up to 1 bonus point max for any additional functionalities. \n",
    "\n",
    "This program should rely heavily on the use of regular expressions, so please make sure to review some introductory material in Learning Python, Programming Python, or some other source before attempting this program.\n",
    "\n",
    "### Be sure to comment your code. In particular, explain what words you are spotting for (and why) and what statement forms you are converting into questions (and why). Also make sure your name, class, etc. is clearly included in the comments.\n",
    "\n",
    "Due: September 21, 2021\n",
    "\n",
    "### ELIZA\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7290b9-78a5-44c2-8f95-c4feb6d84221",
   "metadata": {},
   "source": [
    "## Mandatory Question 1\n",
    "\n",
    "### (1.0 pts) describe the algorithm you have used to solve the problem, specified in a stepwise or point by point fashion. Your introduction should also include identifying information (your name, date, etc.)\n",
    "\n",
    "### Class: AIT 526                                             September 21, 2021\n",
    "#### Professor:  Dr. Duoduo (Lindi) Liao\n",
    "#### Team 5: \n",
    "- Anh \"Tim\" Hien Bach\n",
    "- Robert \"Robb\" Jay Dunlap\n",
    "- Vishnu Lasya Marthala\n",
    "- David Earl Swanson\n",
    "\n",
    "\n",
    "We used the Python programming language to implement a simple version of an Eliza style chatbot—Eliza, originally developed in the 1960s by Dr. Joseph Weizenbaum at MIT, mimicked the responses a Rogerian psychotherapist would say during a therapy session. Our challenge was to redevelop the speech pattern recognition algorithm that determines the key word or phrase in a user’s input sentence and generates an appropriate response.  Ideally, the program will seem to the user like a natural text-based conversation with a Rogerian therapist and not an obvious chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a036e3-9213-474b-b9e2-f3b193141434",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mandatory Question 2\n",
    "\n",
    "### (1.0 pts) give actual examples of program input and output, along with usage instructions, including bonus functionality if you have\n",
    "\n",
    "Instructions:<br>\n",
    "1 - Put the IPYNB and all the other files in the same folder.\n",
    "2 - Run the Team_5_Eliza_final_2021_09_21 file in a Jupyter Notebook/Lab server.\n",
    "3 - Run all the cells in the notebook to be sure all the libraries, text files, and functions are loaded so that the main module can excute.  <br>\n",
    "4 - Scroll to the bottom of the notebook to see the output and interact with our version of Eliza.<br>\n",
    "5 - Type \"quit\" at anytime to exit the program.<br>\n",
    "6 - To rerun the script you will need to press the \"stop\" button in the Jupyter Notebook/Lab and then press the \"play\" button (making sure your cursor is in the last code block).<br>\n",
    "\n",
    "Note - **To use Spacy en_core_web_sm** I had to add the Spacy module to my Conda environment and then I also had to use the following code in a terminal in my environment to add the library: &emsp; `conda install -c conda-forge spacy-model-en_core_web_sm`\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "Below is an actual transcript from our Eliza chatbot:\n",
    "\n",
    "Eliza: Hello! My name is Eliza.  Welcome to my practice.<br>\n",
    "Eliza: I don't want you to think of these sessions as just therapy, but also as an opportunity for self-reflection and growth.<br>\n",
    "Eliza: As this is a safe place, you can stop our session at anytime by typing \"Quit.\"<br>\n",
    "Eliza: What's your name?<br>\n",
    "<br>\n",
    "Patient:  Thomas<br>\n",
    "Eliza: Hello Thomas. How are you feeling today?<br>\n",
    "<br>\n",
    "Thomas:  I'm feeling sad<br>\n",
    "Eliza:  What does \"feeling sad\" mean to you?<br>\n",
    "<br>\n",
    "Thomas:  Sometimes I just can't get out of bed<br>\n",
    "Eliza:  What makes you feel good?<br>\n",
    "<br>\n",
    "Thomas:  I like it when you can dance<br>\n",
    "Eliza:  Can you expand on why you like it when you can dance?<br>\n",
    "<br>\n",
    "Thomas:  I enjoy being with my friends<br>\n",
    "Eliza:  What does \"being with my friends\" mean to you?<br>\n",
    "<br>\n",
    "Thomas:  It helps me remember that I'm connected to people<br>\n",
    "Eliza:  Tell me more about your statement \"I am connected to people\".<br>\n",
    "<br>\n",
    "Thomas:  It's important to have a network of love and support<br>\n",
    "Eliza:  Do you feel you have support?<br>\n",
    "<br>\n",
    "Thomas:  I do most of the time<br>\n",
    "Eliza:  Why do you feel most of the time?<br>\n",
    "<br>\n",
    "Thomas:  Because sometimes I too busy to see them<br>\n",
    "Eliza:  Tell me more about your statement \"sometimes I too busy to see them\".<br>\n",
    "<br>\n",
    "Thomas:  I feel better<br>\n",
    "Eliza:  What makes you feel the best?<br>\n",
    "<br>\n",
    "Thomas:  Talking to you<br>\n",
    "Eliza:  I'm not sure I'm following you, please explain. <br>\n",
    "<br>\n",
    "Thomas:  Thanks you<br>\n",
    "Eliza:  That is interesting, go on.<br>\n",
    "<br>\n",
    "Thomas:  quit<br>\n",
    "Eliza: I hope our session was helpful.  Goodbye<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90ceb5-3ca9-41a2-8aea-dbf31ee95ea1",
   "metadata": {},
   "source": [
    "## Mandatory Question 3\n",
    "\n",
    "### (1.0 pts) describe the algorithm you have used to solve the problem, specified in a stepwise or point by point fashion. Your introduction should also include identifying information (your name, date, etc.)\n",
    "\n",
    "For our program we have 7 main functional components:\n",
    "* Introduction, greeting, and user-name gathering section.\n",
    "* The flow control section that:\n",
    "    * Takes the input from the user and assesses if they response is more than two sentences. If so, this module will respond to the user to nudge them to use single or double sentences. This increases  so that the complexity of the information submitted. and routes it sequentially through our three different response modules until an appropriate response is generated. This section will also quit the exchange with the user if they enter “quit”. \n",
    "    * The “emergency word” response section scans the input for the presence of a short list of key words that could be indicative that the user or someone the user knows could be in trouble and thus we have a response that recommends the user seek professional help immediately.\n",
    "\t* The “standard pattern” response section scans the input for several key words and returns a set response.\n",
    "\t* A more complex “regular expression pattern” response that looks for combinations of words as defined by a list of regex patterns and returns the appropriate response.\n",
    "\t* An “alternative methodology” that uses POS tagging and regular expressions to find more general patterns in the sentences. This function then takes the identified portion of the user’s input and concatenates with a framework response sentence to generate the appropriate response to the user.  \n",
    "\t* A “gibberish/too complex” response that will return a random selection from a predefined list of generic response that will prompt the user to put in another sentence.\n",
    "\n",
    "\n",
    "* The program flow control directs the user input through the response sections until a response if found, or if no patterns match, then the defaut generic gibberish/too-complex response provided to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c7a9f",
   "metadata": {},
   "source": [
    "### Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "282d2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "# To use Spacy en_core_web_sm I had to add the Spacy module to my Conda environment and then I also \n",
    "# had to use the following code in a terminal in my environment to add \n",
    "# the library:   conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser, RegexpChunkParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "# Using the TreebankTagger instead of the Perceptron model in NLTK (TreebankTagger is muuch better)\n",
    "# I found an explanation and the below line of code in this SO post:\n",
    "\n",
    "# https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag\n",
    "treebankTagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n",
    "# os.chdir(r'C:\\Users\\anhhb\\OneDrive\\GMU\\Fall 2021\\AIT 526\\HW\\data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac2e8d",
   "metadata": {},
   "source": [
    "### We decided to \"load\" or read in several dictionaries that support the algorithm in each function rather than hard coding them into the notebook.\n",
    "####    This section loads those dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7306d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code written by the full team\n",
    "# Read in a dictionary of contractions\n",
    "file = open(\"contraction_dictionary.txt\", \"r\")\n",
    "contents = file.read()\n",
    "contractions = ast.literal_eval(contents)\n",
    "file.close\n",
    "\n",
    "# The response if the user uses any one of our \"emergency words\"\n",
    "file = open(\"emergency_words.txt\", \"r\")\n",
    "content = file.read()\n",
    "emergency_words = content.split(\",\")\n",
    "file.close\n",
    "\n",
    "# Read in the emergency response\n",
    "file = open('emergency_response.txt', 'r', encoding = 'utf-8')\n",
    "emergency_response = file.read()\n",
    "file.close\n",
    "\n",
    "# Open the nonspecific responses that will be used when Eliza doesn't understand the patient's statement.\n",
    "file = open(\"gibberish_responses.txt\", \"r\")\n",
    "nonspecific_response = file.readlines()\n",
    "file.close\n",
    "\n",
    "# Open the list of words to be spotted and find synonyms of them to creaste a dictionary of spotting words\n",
    "file = open(\"spotted_words.txt\", \"r\")\n",
    "list_key = file.read()\n",
    "file.close\n",
    "\n",
    "file = open(\"Standard_Responses.txt\", \"r\")\n",
    "standardresponse = file.read()\n",
    "standard_response=ast.literal_eval(standardresponse)\n",
    "file.close\n",
    "\n",
    "\n",
    "file = open(\"reg_exp_pattern.txt\", \"r\")\n",
    "reg_exp_pattern = file.read()\n",
    "reg_exp__pattern=ast.literal_eval(reg_exp_pattern)\n",
    "file.close\n",
    "\n",
    "\n",
    "#Open the list of testing sentence \n",
    "with open(\"test_sentences.txt\", \"r\", encoding=\"utf8\") as input_sentences:\n",
    "    test_sentences_str = input_sentences.readlines()\n",
    "    test_sentences = []\n",
    "    for item in test_sentences_str:\n",
    "        test_sentences.append(item.replace(\"\\n\",\"\"))\n",
    "\n",
    "dic_spotwords={}\n",
    "ListKey = list_key.split()\n",
    "for word in ListKey:\n",
    "    dup_synonyms=[]\n",
    "    for synonym in wordnet.synsets(word):\n",
    "        for lemma in synonym.lemmas():\n",
    "            dup_synonyms.append(lemma.name())\n",
    "\n",
    "    unique_synonyms=[]\n",
    "    for synonym in dup_synonyms:\n",
    "        if synonym not in unique_synonyms:\n",
    "            unique_synonyms.append(synonym)\n",
    "    dic_spotwords[word]=unique_synonyms\n",
    "\n",
    "\n",
    "# This section loads the necessary information for the \"alternate_phrase\" function below. The csv contains:\n",
    "# (1) RE patterns for creating specific chunks from using POS labels\n",
    "# (2) Identifying the portion of the user input that will be \"turned\" back to them in the response\n",
    "# (3) The framework response phrases that the portion identifiied in (2) will be married to\n",
    "\n",
    "input_terms_df = pd.read_csv('input_chunk_terms.csv', \n",
    "                             dtype={'capture_expression':'str', \n",
    "                                     'insertion':'bool', \n",
    "                                     'response_template':'str'}\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41960d0e",
   "metadata": {},
   "source": [
    "### Functions used by the main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a583c3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This section of code written by Vishnu Lasya Marthala and edited by David Swanson \n",
    "#vishnu  \n",
    "def standard_phrase(user_keyword):\n",
    "    \n",
    "    #print('into the loop')\n",
    "    for key_r,value_r in standard_response.items():\n",
    "        #print(key_r)\n",
    "        if(user_keyword ==  key_r ):\n",
    "            #print('userkeyword:',user_keyword,key_r)\n",
    "            return(value_r)\n",
    "    return(\"\")\n",
    "\n",
    "def regex_pattern(sentence):\n",
    "    for key,value in reg_exp__pattern.items():\n",
    "        m=re.search(key,sentence.lower())\n",
    "        ta=None\n",
    "        if(m):\n",
    "            s=random.choice(value)\n",
    "            for rpy in s:\n",
    "                if '{0}' in s:\n",
    "                    ta=m.group(1)\n",
    "                    return(s.format(ta))\n",
    "    return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb0da64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code written by Robb Dunlap\n",
    "\n",
    "# This function counts the occurences of specific POS in a sentence. \n",
    "# It takes in a target sentence and a regular expression and returns the count of \n",
    "# words in the sentence that match the regex. The function is called within the alternate_phrase \n",
    "# function in the function block.\n",
    "\n",
    "def occurence_in_sent(sentence, regex):\n",
    "    regex_of_word = re.compile(regex)\n",
    "    occ_counter = 0\n",
    "    for element in sentence:\n",
    "        if re.match(regex_of_word, element[1]):\n",
    "            occ_counter += 1\n",
    "    return occ_counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function is one of the five different functions for figuring out how we'll respond \n",
    "# the user's input and generate an appropriate response. This specific function uses POS \n",
    "# tagging and a chunker to identify portions of the user's sentence that will then be \n",
    "# pasted into a response template. This method allows us to respond to additional sentences\n",
    "# that we didn't anticipate in our primary phrase identification methods which are based on\n",
    "# word or phrase spotting.\n",
    "\n",
    "# In this function, if it matches one of our patterns then it returns the appropriate \n",
    "# response. If no match is found then the function returns an empty string to the master\n",
    "# control portion of the program.\n",
    "\n",
    "def alternate_phrase(user_sentence):\n",
    "    \n",
    "    # declare the function's response variable as None for program flow control purposes\n",
    "    eliza_response = None\n",
    "    \n",
    "    # This section of the function expands contractions into their component (two or more) words\n",
    "    # and reassembles them as a complete string. We don't tokenize the user's input in the \n",
    "    # alternative_phrase function because we are going to grab a portion of it for the response.\n",
    "    # If we used the NLTK Tokenizer then we might have sentences that don't look like real English\n",
    "    # due to the stemming. The output from this block is \"new_sentence\".\n",
    "    \n",
    "    new_sentence = \"\"\n",
    "    new_word = \"\"\n",
    "    for word in user_sentence.split():\n",
    "        for key, root in contractions.items():\n",
    "            if key == word:\n",
    "                word = root\n",
    "        new_word = word+\" \"\n",
    "        new_word += \" \"\n",
    "        new_sentence += new_word\n",
    "    \n",
    "    # Strips off the extra space added to the end of the sentence\n",
    "    new_sentence = new_sentence[:-2]\n",
    "\n",
    "    # This creates a list of words from the sentence where we just removed the contractions\n",
    "    split_sent = new_sentence.split()\n",
    "    sent_words_wo_punct = [w.strip(punctuation) for w in split_sent]\n",
    "\n",
    "    # We use the Treebank Tager to create a list of sets of each word and it's POS label. \n",
    "    # It does a much better job than the nltk.pos_tag() function which frequently mis-identifies\n",
    "    # parts-of-speech.\n",
    "    \n",
    "    tokens_tag = treebankTagger.tag(sent_words_wo_punct)\n",
    "\n",
    "    # Sentences that have more than 4 verbs or nouns are generally too complex for the\n",
    "    # alternative_phrase function to be able to select the appropriate portion of the input \n",
    "    # sentence for generating an appropriate response. So, this block of code counts \n",
    "    # the occurences of nouns or verbs (any type) and will return a blank string to the master\n",
    "    # control portion of the program if there are more than 4 occurences of either. As a \n",
    "    # result, the the main loop will call the \"gibberish/too complicated\" to prompt the user \n",
    "    # to input another sentence.\n",
    "    \n",
    "    noun_pos_regex = \"NN.?\"\n",
    "    count_of_nouns = occurence_in_sent(tokens_tag, noun_pos_regex)\n",
    "\n",
    "    verb_pos_regex = \"VB.?\"\n",
    "    count_of_verbs = occurence_in_sent(tokens_tag, verb_pos_regex)\n",
    "\n",
    "    if count_of_nouns > 4 or count_of_verbs > 4:\n",
    "        return eliza_response\n",
    "\n",
    "    # This block of code iterates through the list or regular expressions that look for specific\n",
    "    # sequences of POS labels to find a match in the user's sentence. If a match is found, then \n",
    "    # the loop breaks and the code flow moves on to the response generation section below. If a\n",
    "    # match isn't found \n",
    "   \n",
    "    # counter for keeping track of progress through the dataframe of patterns \n",
    "    # and response frameworks\n",
    "    index_in_df = -1\n",
    "\n",
    "    # a for-loop to evaluate each POS pattern (AKA: capture_expressions) against the user's input \n",
    "    # sentence. When a pattern is found then the variable \"pattern_found\" is set to 1 and the flow \n",
    "    # through the for-loop breaks. If a pattern isn't found then the loop exits normally but the \n",
    "    # \"pattern_found\" variable is still set at 0. The \"pattern_found\" variable will be used below \n",
    "    # for outputting the appropriate response or sending back an empty string to the program flow \n",
    "    # control logic.\n",
    "    \n",
    "    for statement in input_terms_df['capture_expression']:\n",
    "        pattern_found = 0\n",
    "        index_in_df+=1\n",
    "        phrase_pattern = statement\n",
    "        \n",
    "        # The chunker will find the portions of the sentence that match the phrase pattern\n",
    "        # if it is present\n",
    "        chunker = RegexpParser(phrase_pattern)\n",
    "        \n",
    "#        # original code - it's not necessary to serially pass the output, can just use tokens_tag\n",
    "#        # each time because I'm not building a fully-fleshed out tree like I did in development\n",
    "#         if index_in_df<2:\n",
    "#             output = chunker.parse(tokens_tag)\n",
    "#         else:\n",
    "#             output = chunker.parse(output)  \n",
    "        \n",
    "        # the phrase_pattern is passed against the list of POS labelled words\n",
    "        output = chunker.parse(tokens_tag)\n",
    "\n",
    "        # this portion of the for loop checks to see if the phrase_pattern was found in the \n",
    "        # sentence. It breaks the for-loop if it did find a match.\n",
    "        \n",
    "        key_word = []\n",
    "        for item in output:\n",
    "            if isinstance(item, nltk.tree.Tree):\n",
    "                pattern_found = 1\n",
    "                regex_desired_word = re.compile(input_terms_df.iloc[index_in_df,3])\n",
    "                for thing in item:\n",
    "                    if re.match(regex_desired_word, thing[1]):     \n",
    "                        key_word = thing[0]\n",
    "        if pattern_found == 1:\n",
    "            break\n",
    "\n",
    "    # the below section captures the desired portion of the sentence so it can be \"turned\"\n",
    "    # into the response, mimicking the Rogerian technique.\n",
    "\n",
    "    position_in_sent_word = 0\n",
    "    counter = 0\n",
    "    for element in sent_words_wo_punct:\n",
    "        if key_word == element:\n",
    "            counter = position_in_sent_word\n",
    "        position_in_sent_word += 1\n",
    "\n",
    "    offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "    turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "\n",
    "    # This section appends or inserts the flipped portion of the input text to the appropriate template \n",
    "    if pattern_found == 1:\n",
    "        \n",
    "        # For some of the sentence frameworks, the \"turned\" portion of the user's input\n",
    "        # is inserted into the framework. For the other frameworks, the \"turned\" portion\n",
    "        # is appended to the end. The if-statement below carries out the insertion function\n",
    "        # while the else-statement carries out the append function.\n",
    "        \n",
    "        if not input_terms_df.iloc[index_in_df,1]:\n",
    "            offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "            turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "            if not turned_portion_of_sent:\n",
    "                return eliza_response\n",
    "            else:\n",
    "                temp_holder = \"\"\n",
    "                separator = \" \"\n",
    "                temp_holder = separator.join(turned_portion_of_sent)\n",
    "                question_mark = \"?\"\n",
    "                temp_holder.join(question_mark)\n",
    "                eliza_response = input_terms_df.iloc[index_in_df,2]+\" \"+temp_holder+\"?\"\n",
    "        else:\n",
    "            offset_in_sentence = counter + input_terms_df.iloc[index_in_df,4]\n",
    "            turned_portion_of_sent = sent_words_wo_punct[offset_in_sentence:]\n",
    "            if not turned_portion_of_sent:\n",
    "                return eliza_response\n",
    "            else:\n",
    "                temp_holder = \"\"\n",
    "                separator = \" \"\n",
    "                temp_holder = separator.join(turned_portion_of_sent)\n",
    "                turned_portion_of_sent = str(temp_holder)\n",
    "                eliza_response = input_terms_df.iloc[index_in_df,2].replace(\"XXYYMM\",turned_portion_of_sent)\n",
    "        return eliza_response    \n",
    "\n",
    "    # This returns an empty string to the calling function to indicate that a pattern match wasn't\n",
    "    # found. \n",
    "    return eliza_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca44a1",
   "metadata": {},
   "source": [
    "### Eliza welcomes the patient, sets the tone for the conversation, and establishes an end of session codeword \"quit.\"\n",
    "#### We decided that Eliza could asked for the patient's name which is used to tag the patient's inputs lines as well as a salutation.\n",
    "Code written by David Swanson and Vishnu Lasya Marthala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6fb66",
   "metadata": {},
   "source": [
    "#### The following code is a loop that\n",
    "1. Tokenizes the sentances in the response\n",
    "2. If the patient talks too long (more than two sentences) Eliza asks in several ways for the patient to slow down \n",
    "3. Tokenizes the words of each sentence to be sent to four functions in sequence.\n",
    "    1. Has the patient made a statement that is dangerous to themselves of the theoropist (aka emergency words)? \n",
    "                     - If so, instruct the patient to seek help and end the session \n",
    "    2. Can Eliza respond using spotted words?\n",
    "    3. Can a sophisticated regular expression algorithm make a response?\n",
    "    4. If all these fail, Eliza assumes the answer was gibberish and randomly selects a response for the patient to talk more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce7563fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza: Hello! My name is Eliza.  Welcome to my practice.\n",
      "Eliza: I don't want you to think of these sessions as just therapy, but also as an opportunity for self-reflection and growth.\n",
      "Eliza: As this is a safe place, you can stop our session at anytime by typing \"Quit.\"\n",
      "Eliza: What's your name?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Patient:  Thomas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza: Hello Thomas. How are you feeling today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  I'm feeling sad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  What does \"feeling sad\" mean to you?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  Sometimes I just can't get out of bed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  What makes you feel good?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  I like it when I can dance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  Can you expand on why you like it when I can dance?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  I like it when you can dance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  Can you expand on why you like it when you can dance?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  I enjoy being with my friends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  What does \"being with my friends\" mean to you?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  It helps me remember that I'm connected to people\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  Tell me more about your statement \"I am connected to people\".\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  It's important to have a network of love and support\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  Do you feel you have support?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  I do most of the time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  Why do you feel most of the time?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  Because sometimes I too busy to see them\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  Tell me more about your statement \"sometimes I too busy to see them\".\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  I feel better\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  What makes you feel the best?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  Talking to you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  I'm not sure I'm following you, please explain. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  Thanks you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza:  That is interesting, go on.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Thomas:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliza: I hope our session was helpful.  Goodbye\n"
     ]
    }
   ],
   "source": [
    "# This section of code written by David Swanson\n",
    "\n",
    "print(\"Eliza: Hello! My name is Eliza.  Welcome to my practice.\")\n",
    "print(\"Eliza: I don't want you to think of these sessions as just therapy, but also as an opportunity for self-reflection and growth.\")\n",
    "print('Eliza: As this is a safe place, you can stop our session at anytime by typing \"Quit.\"')\n",
    "\n",
    "# Get the patient's name\n",
    "print(\"Eliza: What's your name?\")\n",
    "print(\"\")\n",
    "user_input = input(\"Patient: \")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(user_input)  \n",
    "usernames=[e for e in doc.ents if e.label_ == 'PERSON']\n",
    "\n",
    "if doc.ents==():\n",
    "    patientName = \"Patient: \"\n",
    "    print(\"Eliza: That's ok, we don't need to use names. So how are you feeling today?\")\n",
    "else:\n",
    "    patientName=usernames[0].text\n",
    "    print('Eliza: Hello '+ patientName +'. '+\"How are you feeling today?\")\n",
    "\n",
    "# Set variables used in the while-loop \n",
    "loopAgain = True \n",
    "lengthyReponse1 = 0\n",
    "lengthyReponse2 = 0\n",
    "\n",
    "loop = 0\n",
    "\n",
    "# Top of the while-loop. This loop is where the patients input is analyzed and the appropriate response \n",
    "# is selected and displayed for them\n",
    "\n",
    "while loopAgain == True:\n",
    "\n",
    "    # Get the patient's input \n",
    "    print(\"\")\n",
    "    user_input = input(patientName+\": \")\n",
    "        \n",
    "        \n",
    "    # First check the patient's response to see if they want to end the session.\n",
    "    if (user_input.lower() == \"quit\"):\n",
    "        print(\"Eliza: I hope our session was helpful.  Goodbye\")\n",
    "        loopAgain = False\n",
    "        break\n",
    "        \n",
    "    # Start a counter to cycle through some random responses if the patients enters too many sentences at a time\n",
    "    lengthyReponse = 0\n",
    "    \n",
    "    # Separate the response into individual sentences\n",
    "    PatientResponse = nltk.sent_tokenize(user_input)\n",
    "    # print(PatientResponse)\n",
    "    \n",
    "    # Send the patient's response word by word to check for emergency words\n",
    "    # Code written by Anh \"Tim\" Hien Bach\n",
    "\n",
    "    for sentences in PatientResponse:\n",
    "        PatientWords = nltk.word_tokenize(sentences)\n",
    "        for words in PatientWords: \n",
    "            for EmWord in emergency_words:\n",
    "                # If any emergency word is found, return with a standard cautionary response\n",
    "                if re.search(EmWord, words):\n",
    "                    print('Eliza:', emergency_response)\n",
    "                    loopAgain = False\n",
    "                    break\n",
    "            if loopAgain == False:\n",
    "                break\n",
    "        if loopAgain == False:\n",
    "            break\n",
    "\n",
    "    if loopAgain == False:\n",
    "        break\n",
    "            \n",
    "    # If the response is more than 2 sentences  \n",
    "    if loopAgain == True and len(PatientResponse) > 2:\n",
    "        \n",
    "        # Cycle through two responses if the patient inputs 3 or more sentences \n",
    "        if(len(PatientResponse)==3): \n",
    "            lengthyReponse1 = lengthyReponse1 + 1\n",
    "            if(lengthyReponse == 1):\n",
    "                print(\"Eliza: Let's go slower and take things one at a time. What is on your mind?\")\n",
    "            if(lengthyReponse > 1):\n",
    "                print(\"Eliza: We need to go slower to help you reflect. Let's take things one at a time. What is on your mind?\")\n",
    "                lengthy_response = 0\n",
    "                \n",
    "        # Cycle through three three responses if the patient inputs more than 3 sentences \n",
    "        else: \n",
    "            lengthyReponse1 = lengthyReponse1 + 1\n",
    "            if(lengthyReponse == 1):\n",
    "                print(\"Eliza: That's a lot to unpack. Let's try again with shorter thoughts.\")\n",
    "            if(lengthyReponse == 2):\n",
    "                print(\"Eliza: To help you we need to focus on one thing at a time. Please try again.\")\n",
    "            if(lengthyReponse > 2):\n",
    "                print(\"Eliza: You have alot on your mind. Let's start again with just your first thought.\")\n",
    "                lengthyReponse = 0\n",
    "\n",
    "    # This section responds to patient input if it's one or two sentences. If the patient gives a two sentence \n",
    "    # response, then we respond to the second sentence            \n",
    "\n",
    "    sentence = PatientResponse[-1]\n",
    "\n",
    "    # Tokenize the updated sentence into words\n",
    "    tokenizer = RegexpTokenizer (r'\\w+')\n",
    "    words_only = tokenizer.tokenize(sentence)\n",
    "\n",
    "    # Set flow_control to 0 to use in directing the program flow through the response generator functions\n",
    "    flow_control = 0\n",
    "\n",
    "    # Send the patient's sentence to a word spotter routine to see if a match and appropriate\n",
    "    # response can be found\n",
    "    for token in words_only:\n",
    "        for key, value in dic_spotwords.items():\n",
    "            if token in value:\n",
    "                ElizaResponse = standard_phrase(key)\n",
    "                print(\"Eliza: \", ElizaResponse)\n",
    "                flow_control = 1\n",
    "                break\n",
    "        if flow_control == 1:\n",
    "            break\n",
    "       \n",
    "    # Send the patient's sentence through a series of regexs to see if a match and appropriate\n",
    "    # response can be found\n",
    "    if flow_control == 0:\n",
    "        ElizaResponse = regex_pattern(sentence)\n",
    "        if len(ElizaResponse) > 0:\n",
    "            print(\"Eliza: \", ElizaResponse)\n",
    "            flow_control = 1\n",
    "\n",
    "\n",
    "    # If the both the word-spotter and the regex routines fails to provide a response then the sentence\n",
    "    # is sent through this alternative function. The function receives the input string untokenized \n",
    "    # because it processes the input differently. The specific method is detailed in function code block.\n",
    "    if flow_control == 0: \n",
    "        ElizaResponse = alternate_phrase(sentence)\n",
    "        if ElizaResponse is not None:\n",
    "            print(\"Eliza: \", ElizaResponse)\n",
    "            flow_control = 1\n",
    "    \n",
    "    # Those don't provide a response, it is gibberish so send a random response\n",
    "    if flow_control == 0: \n",
    "        print(\"Eliza: \", random.choice(nonspecific_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
